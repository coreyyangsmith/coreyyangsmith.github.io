---
title: "GazeTrack"
excerpt: "Eye Tracking in the Browser"
collection: portfolio
---
ğ—šğ—®ğ˜‡ğ—²ğ—§ğ—¿ğ—®ğ—°ğ—¸ is a ğ—³ğ˜‚ğ—¹ğ—¹-ğ˜€ğ˜ğ—®ğ—°ğ—¸ ğ˜„ğ—²ğ—¯ ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» that enables ğ—°ğ—¹ğ—¶ğ—²ğ—»ğ˜-ğ˜€ğ—¶ğ—±ğ—² ğ—²ğ˜†ğ—² ğ˜ğ—¿ğ—®ğ—°ğ—¸ğ—¶ğ—»ğ—´ for video-based user studies. I ğ—¹ğ—²ğ—± ğ˜ğ—µğ—² ğ˜ğ—²ğ—®ğ—º and ğ—±ğ—²ğ˜ƒğ—²ğ—¹ğ—¼ğ—½ğ—²ğ—± ğ˜ğ—µğ—² ğ— ğ—Ÿ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—§ğ—²ğ—»ğ˜€ğ—¼ğ—¿ğ—™ğ—¹ğ—¼ğ˜„.ğ—·ğ˜€ for in-browser face and eye detection, training a lightweight ğ—°ğ˜‚ğ˜€ğ˜ğ—¼ğ—º ğ—–ğ—¡ğ—¡ ğ—¼ğ—» ğ—® ğ—°ğ˜‚ğ—¿ğ—®ğ˜ğ—²ğ—± ğ—±ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜. The model is ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—²ğ—± ğ—¶ğ—» ğ—¿ğ—²ğ—®ğ—¹-ğ˜ğ—¶ğ—ºğ—² through user calibration to improve gaze accuracy (~80%). I also ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—²ğ—± ğ—µğ—²ğ—®ğ˜ğ—ºğ—®ğ—½ ğ˜ƒğ—¶ğ˜€ğ˜‚ğ—®ğ—¹ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€ over video content using ğ— ğ—®ğ˜ğ—½ğ—¹ğ—¼ğ˜ğ—¹ğ—¶ğ—¯ and ğ—™ğ—™ğ—ºğ—½ğ—²ğ—´, allowing researchers to easily analyze gaze distribution. The platform is fully containerized using ğ——ğ—¼ğ—°ğ—¸ğ—²ğ—¿ for straightforward deployment.

<iframe width="560" height="315" src="https://www.youtube.com/embed/qNN5U05Kg_c?si=QNRuCT7odf9bUGNs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---

Due to an NDA we cannot share the codebase for this project.

Technologies used: React, Tensorflow.js, Node.js, Express.js

Collaborators: [Israel Robles](https://www.linkedin.com/in/oakisr/), [Hao Liu](https://www.linkedin.com/in/haojamesliu/), [Eric Diep](https://www.linkedin.com/in/ediep/)